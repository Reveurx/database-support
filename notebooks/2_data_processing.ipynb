{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adda0966-b809-4ba9-9d86-9603e0267d2e",
      "metadata": {
        "id": "adda0966-b809-4ba9-9d86-9603e0267d2e"
      },
      "outputs": [],
      "source": [
        "# Функция для парсинга XML\n",
        "def parse_xml_to_df(file_path):\n",
        "    try:\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "        data = []\n",
        "        for row in root.findall('row'):\n",
        "            data.append(row.attrib)\n",
        "        df = pd.DataFrame(data)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке {file_path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Функция для подготовки DataFrame\n",
        "def prepare_df_auto(df, archive_version):\n",
        "    df = df.copy()\n",
        "    df.columns = [col.strip() for col in df.columns]\n",
        "\n",
        "    if \"Id\" in df.columns:\n",
        "        df.rename(columns={\"Id\": \"OriginalId\"}, inplace=True)\n",
        "    df[\"OriginalId\"] = df[\"OriginalId\"].astype(int)\n",
        "\n",
        "    date_columns = [col for col in df.columns if \"Date\" in col]\n",
        "    for col in date_columns:\n",
        "        df[col] = pd.to_datetime(df[col], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
        "\n",
        "    df[\"ArchiveVersion\"] = archive_version\n",
        "    return df\n",
        "\n",
        "# Настройки подключения к БД                                                                     -- Вставить свои параметры\n",
        "DB_CONFIG = {\n",
        "    'user': 'postgres',\n",
        "    'password': '___',\n",
        "    'host': 'localhost',\n",
        "    'port': '5432',\n",
        "    'database': 'postgres'\n",
        "}\n",
        "engine = create_engine(f'postgresql+psycopg2://{DB_CONFIG[\"user\"]}:{DB_CONFIG[\"password\"]}@{DB_CONFIG[\"host\"]}:{DB_CONFIG[\"port\"]}/{DB_CONFIG[\"database\"]}')\n",
        "\n",
        "# Словарь для преобразования типов\n",
        "SQLALCHEMY_TYPE_MAP = {\n",
        "    'integer': Integer(),\n",
        "    'smallint': SmallInteger(),\n",
        "    'character varying': String(),\n",
        "    'text': Text(),\n",
        "    'timestamp without time zone': DateTime(),\n",
        "    'int': Integer(),\n",
        "    'nvarchar': String(),\n",
        "    'datetime': DateTime(),\n",
        "    'tinyint': SmallInteger(),\n",
        "    'bit': Boolean(),\n",
        "    'date': Date(),\n",
        "    'uniqueidentifier': UUID(),\n",
        "    'varchar(32)': String(32)\n",
        "}\n",
        "\n",
        "def get_table_schema(table_name):\n",
        "    \"\"\"Получить схему таблицы из БД\"\"\"\n",
        "    query = f\"\"\"\n",
        "    SELECT column_name, data_type\n",
        "    FROM information_schema.columns\n",
        "    WHERE table_schema = 'public' AND table_name = '{table_name.lower()}';\n",
        "    \"\"\"\n",
        "    return pd.read_sql(query, engine)\n",
        "\n",
        "def prepare_data_types(df, table_name):\n",
        "    \"\"\"Приведение типов данных в соответствии со схемой таблицы\"\"\"\n",
        "    df_schema = get_table_schema(table_name)\n",
        "    dtype_dict = {row['column_name']: row['data_type'] for _, row in df_schema.iterrows()}\n",
        "\n",
        "    for col, db_type in dtype_dict.items():\n",
        "        if col in df.columns:\n",
        "            if db_type in ['integer', 'smallint', 'int']:\n",
        "                df[col] = df[col].astype('Int64')\n",
        "            elif db_type in ['timestamp without time zone', 'datetime']:\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "            elif db_type == 'bit':\n",
        "                df[col] = df[col].astype(bool)\n",
        "            elif db_type in ['text', 'character varying', 'nvarchar', 'varchar(32)']:\n",
        "                df[col] = df[col].astype(str).where(df[col].notna(), None)\n",
        "            elif db_type == 'date':\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
        "            elif db_type == 'uniqueidentifier':\n",
        "                df[col] = df[col].astype(str)\n",
        "    return df\n",
        "\n",
        "def get_sqlalchemy_dtype_dict(table_name):\n",
        "    \"\"\"Получить словарь типов SQLAlchemy для таблицы\"\"\"\n",
        "    df_schema = get_table_schema(table_name)\n",
        "    return {\n",
        "        col: SQLALCHEMY_TYPE_MAP[db_type]\n",
        "        for col, db_type in zip(df_schema['column_name'], df_schema['data_type'])\n",
        "        if db_type in SQLALCHEMY_TYPE_MAP\n",
        "    }\n",
        "\n",
        "def load_data_to_db(df, table_name):\n",
        "    \"\"\"Загрузка данных в таблицу БД\"\"\"\n",
        "    sqlalchemy_dtype_dict = get_sqlalchemy_dtype_dict(table_name)\n",
        "    df.to_sql(\n",
        "        name=table_name.lower(),\n",
        "        con=engine,\n",
        "        if_exists='append',\n",
        "        index=False,\n",
        "        dtype=sqlalchemy_dtype_dict\n",
        "    )\n",
        "\n",
        "# Основные директории\n",
        "BASE_DIR = r\"C:\\ваш\\путь\\к_проекту\"\n",
        "ARCHIVE_DIRS = {\n",
        "    'meta': os.path.join(BASE_DIR, \"dba.meta.stackexchange.com\"),\n",
        "    'main': os.path.join(BASE_DIR, \"dba.stackexchange.com\")\n",
        "}\n",
        "\n",
        "def process_table(table_name):\n",
        "    \"\"\"Обработка таблицы с точным сохранением оригинальной логики\"\"\"\n",
        "    print(f\"\\nНачата обработка таблицы {table_name}...\")\n",
        "\n",
        "    # Загрузка данных из XML\n",
        "    file1 = os.path.join(ARCHIVE_DIRS['meta'], f\"{table_name}.xml\")\n",
        "    file2 = os.path.join(ARCHIVE_DIRS['main'], f\"{table_name}.xml\")\n",
        "\n",
        "    df1 = parse_xml_to_df(file1)\n",
        "    df2 = parse_xml_to_df(file2)\n",
        "\n",
        "    # Подготовка данных\n",
        "    df1_prepared = prepare_df_auto(df1, 1)\n",
        "    df2_prepared = prepare_df_auto(df2, 2)\n",
        "    df_combined = pd.concat([df1_prepared, df2_prepared], ignore_index=True)\n",
        "\n",
        "    # Обработка для конкретных таблиц\n",
        "    if table_name.lower() == 'users':\n",
        "        # Для Users просто преобразуем типы\n",
        "        df_combined.columns = [col.lower() for col in df_combined.columns]\n",
        "\n",
        "    elif table_name.lower() == 'badges':\n",
        "        # Обработка Badges\n",
        "        df_users_db = pd.read_sql(\"SELECT id, originalid, archiveversion FROM users\", engine)\n",
        "        df_users_db.columns = [col.lower() for col in df_users_db.columns]\n",
        "\n",
        "        df_combined['UserId'] = df_combined['UserId'].astype('Int64')\n",
        "        df_combined.columns = [col.lower() for col in df_combined.columns]\n",
        "\n",
        "        df_combined = df_combined.merge(\n",
        "            df_users_db,\n",
        "            left_on=['userid', 'archiveversion'],\n",
        "            right_on=['originalid', 'archiveversion'],\n",
        "            how='left'\n",
        "        )\n",
        "        df_combined['userid'] = df_combined['id'].astype('Int64')\n",
        "        df_combined.drop(columns=['originalid_x', 'archiveversion', 'id', 'originalid_y'], inplace=True, errors='ignore')\n",
        "\n",
        "    elif table_name.lower() == 'posts':\n",
        "        # Обработка Posts\n",
        "        df_users_db = pd.read_sql(\"SELECT id, originalid, archiveversion FROM users\", engine)\n",
        "        df_users_db.columns = [col.lower() for col in df_users_db.columns]\n",
        "\n",
        "        for col in df_combined.columns:\n",
        "            if 'Id' in col and df_combined[col].dtype == 'object':\n",
        "                df_combined[col] = df_combined[col].astype('Int64')\n",
        "\n",
        "        df_combined.columns = [col.lower() for col in df_combined.columns]\n",
        "\n",
        "        # Обработка owneruserid\n",
        "        df_combined = df_combined.merge(\n",
        "            df_users_db,\n",
        "            left_on=['owneruserid', 'archiveversion'],\n",
        "            right_on=['originalid', 'archiveversion'],\n",
        "            how='left'\n",
        "        )\n",
        "        df_combined['owneruserid'] = df_combined['id'].astype('Int64')\n",
        "        df_combined.drop(columns=['id', 'originalid_y'], inplace=True)\n",
        "        df_combined.rename(columns={'originalid_x': 'originalid'}, inplace=True)\n",
        "\n",
        "        # Обработка lasteditoruserid\n",
        "        df_combined = df_combined.merge(\n",
        "            df_users_db,\n",
        "            left_on=['lasteditoruserid', 'archiveversion'],\n",
        "            right_on=['originalid', 'archiveversion'],\n",
        "            how='left'\n",
        "        )\n",
        "        df_combined['lasteditoruserid'] = df_combined['id'].astype('Int64')\n",
        "        df_combined.drop(columns=['id', 'originalid_y'], inplace=True)\n",
        "        df_combined.rename(columns={'originalid_x': 'originalid'}, inplace=True)\n",
        "\n",
        "        # Исправление строки WHERE AcceptedAnswerId = 338217 и не имеет внешнего Id\n",
        "        if 'acceptedanswerid' in df_combined.columns:\n",
        "            df_combined.loc[df_combined['acceptedanswerid'] == 338217, 'acceptedanswerid'] = None\n",
        "\n",
        "    elif table_name.lower() == 'tags':\n",
        "        # Обработка Tags\n",
        "        df_posts_db = pd.read_sql(\"SELECT id, originalid, archiveversion FROM posts\", engine)\n",
        "        df_posts_db.columns = [col.lower() for col in df_posts_db.columns]\n",
        "\n",
        "        for col in df_combined.columns:\n",
        "            if 'Id' in col and df_combined[col].dtype == 'object':\n",
        "                df_combined[col] = df_combined[col].astype('Int64')\n",
        "\n",
        "        df_combined.columns = [col.lower() for col in df_combined.columns]\n",
        "\n",
        "        # Обработка excerptpostid\n",
        "        df_combined = df_combined.merge(\n",
        "            df_posts_db,\n",
        "            left_on=['excerptpostid', 'archiveversion'],\n",
        "            right_on=['originalid', 'archiveversion'],\n",
        "            how='left'\n",
        "        )\n",
        "        df_combined['excerptpostid'] = df_combined['id'].astype('Int64')\n",
        "        df_combined.drop(columns=['id', 'originalid_y'], inplace=True)\n",
        "        df_combined.rename(columns={'originalid_x': 'originalid'}, inplace=True)\n",
        "\n",
        "        # Обработка wikipostid\n",
        "        df_combined = df_combined.merge(\n",
        "            df_posts_db,\n",
        "            left_on=['wikipostid', 'archiveversion'],\n",
        "            right_on=['originalid', 'archiveversion'],\n",
        "            how='left'\n",
        "        )\n",
        "        df_combined['wikipostid'] = df_combined['id'].astype('Int64')\n",
        "        df_combined.drop(columns=['originalid_x', 'archiveversion', 'id', 'originalid_y'], inplace=True)\n",
        "\n",
        "    elif table_name.lower() == 'postlinks':\n",
        "        # Обработка PostLinks\n",
        "        df_posts_db = pd.read_sql(\"SELECT id, originalid, archiveversion FROM posts\", engine)\n",
        "        df_posts_db.columns = [col.lower() for col in df_posts_db.columns]\n",
        "\n",
        "        for col in df_combined.columns:\n",
        "            if 'Id' in col and df_combined[col].dtype == 'object':\n",
        "                df_combined[col] = df_combined[col].astype('Int64')\n",
        "\n",
        "        df_combined.columns = [col.lower() for col in df_combined.columns]\n",
        "\n",
        "        # Обработка postid\n",
        "        df_combined = df_combined.merge(\n",
        "            df_posts_db,\n",
        "            left_on=['postid', 'archiveversion'],\n",
        "            right_on=['originalid', 'archiveversion'],\n",
        "            how='left'\n",
        "        )\n",
        "        df_combined['postid'] = df_combined['id'].astype('Int64')\n",
        "        df_combined.drop(columns=['id', 'originalid_y'], inplace=True)\n",
        "        df_combined.rename(columns={'originalid_x': 'originalid'}, inplace=True)\n",
        "\n",
        "        # Обработка relatedpostid\n",
        "        df_combined = df_combined.merge(\n",
        "            df_posts_db,\n",
        "            left_on=['relatedpostid', 'archiveversion'],\n",
        "            right_on=['originalid', 'archiveversion'],\n",
        "            how='left'\n",
        "        )\n",
        "        df_combined['relatedpostid'] = df_combined['id'].astype('Int64')\n",
        "        df_combined.drop(columns=['originalid_x', 'archiveversion', 'id', 'originalid_y'], inplace=True)\n",
        "\n",
        "    elif table_name.lower() in ['posthistory', 'comments', 'votes']:\n",
        "        # Общая логика для PostHistory, Comments и Votes\n",
        "        df_users_db = pd.read_sql(\"SELECT id, originalid, archiveversion FROM users\", engine)\n",
        "        df_users_db.columns = [col.lower() for col in df_users_db.columns]\n",
        "\n",
        "        df_posts_db = pd.read_sql(\"SELECT id, originalid, archiveversion FROM posts\", engine)\n",
        "        df_posts_db.columns = [col.lower() for col in df_posts_db.columns]\n",
        "\n",
        "        for col in df_combined.columns:\n",
        "            if 'Id' in col and df_combined[col].dtype == 'object':\n",
        "                df_combined[col] = df_combined[col].astype('Int64')\n",
        "\n",
        "        df_combined.columns = [col.lower() for col in df_combined.columns]\n",
        "\n",
        "        # Обработка userid\n",
        "        if 'userid' in df_combined.columns:\n",
        "            df_combined = df_combined.merge(\n",
        "                df_users_db,\n",
        "                left_on=['userid', 'archiveversion'],\n",
        "                right_on=['originalid', 'archiveversion'],\n",
        "                how='left'\n",
        "            )\n",
        "            df_combined['userid'] = df_combined['id'].astype('Int64')\n",
        "            df_combined.drop(columns=['id', 'originalid_y'], inplace=True)\n",
        "            df_combined.rename(columns={'originalid_x': 'originalid'}, inplace=True)\n",
        "\n",
        "        # Обработка postid\n",
        "        if 'postid' in df_combined.columns:\n",
        "            df_combined = df_combined.merge(\n",
        "                df_posts_db,\n",
        "                left_on=['postid', 'archiveversion'],\n",
        "                right_on=['originalid', 'archiveversion'],\n",
        "                how='left'\n",
        "            )\n",
        "            df_combined['postid'] = df_combined['id'].astype('Int64')\n",
        "            df_combined.drop(columns=['originalid_x', 'archiveversion', 'id', 'originalid_y'], inplace=True)\n",
        "\n",
        "    # Приведение типов данных\n",
        "    df_combined = prepare_data_types(df_combined, table_name)\n",
        "\n",
        "    # Загрузка в БД\n",
        "    load_data_to_db(df_combined, table_name)\n",
        "    print(f\"Таблица {table_name} успешно обработана и загружена в БД\")\n",
        "\n",
        "    return df_combined\n",
        "\n",
        "# Запуск обработки всех таблиц\n",
        "if __name__ == \"__main__\":\n",
        "    # Порядок обработки важен из-за зависимостей между таблицами\n",
        "    tables_to_process = [\n",
        "        'Users',    # Должна быть первой, так как другие таблицы ссылаются на Users\n",
        "        'Badges',   # Зависит от Users\n",
        "        'Posts',    # Зависит от Users\n",
        "        'Tags',     # Зависит от Posts\n",
        "        'PostLinks', # Зависит от Posts\n",
        "        'PostHistory', # Зависит от Users и Posts\n",
        "        'Comments',  # Зависит от Users и Posts\n",
        "        'Votes'     # Зависит от Users и Posts\n",
        "    ]\n",
        "\n",
        "    for table in tables_to_process:\n",
        "        try:\n",
        "            process_table(table)\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке таблицы {table}: {e}\")\n",
        "\n",
        "    print(\"\\nВсе таблицы успешно обработаны и загружены в БД\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}